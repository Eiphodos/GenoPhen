model:
  class: RobertaForMaskedLM
  n_attention_heads: 4
  n_hidden_layers: 4
  hidden_size: 512