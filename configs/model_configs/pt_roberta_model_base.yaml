model:
  class: RobertaForMaskedLM
  n_attention_heads: 4
  n_hidden_layers: 4
  hidden_size: 512
tokenizer:
  use_pretrained: False
  class: RobertaTokenizer
  columns_for_corpus:
    - AMR_genotypes_core