model:
  class: RobertaHierForMaskedLM
  n_attention_heads: 4
  n_hidden_layers: 4
  hidden_size: 512
tokenizer:
  use_pretrained: False
  class: RobertaTokenizer
  columns_for_corpus:
    - AMR_genotypes_core
    - Hierarchy_data
  max_len: 256
  special_token_list:
    - <s>
    - <pad>
    - </s>
    - <mask>
    - <unk>
    - <gpsep>